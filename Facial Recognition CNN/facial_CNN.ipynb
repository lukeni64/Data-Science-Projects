{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project #2: Facial Verification"
      ],
      "metadata": {
        "id": "CB_aBy3ASjn1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Necessary Libraries"
      ],
      "metadata": {
        "id": "PBdb03GHSabN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YptXzFm8IgJx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5635dab1-8f51-4d6a-fee2-57e74411d0db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: facenet_pytorch in /usr/local/lib/python3.12/dist-packages (2.6.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from facenet_pytorch) (1.26.4)\n",
            "Requirement already satisfied: Pillow<10.3.0,>=10.2.0 in /usr/local/lib/python3.12/dist-packages (from facenet_pytorch) (10.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from facenet_pytorch) (2.32.4)\n",
            "Requirement already satisfied: torch<2.3.0,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from facenet_pytorch) (2.2.2)\n",
            "Requirement already satisfied: torchvision<0.18.0,>=0.17.0 in /usr/local/lib/python3.12/dist-packages (from facenet_pytorch) (0.17.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from facenet_pytorch) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet_pytorch) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet_pytorch) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet_pytorch) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet_pytorch) (2025.10.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet_pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet_pytorch) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet_pytorch) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch<2.3.0,>=2.2.0->facenet_pytorch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "!pip install facenet_pytorch\n",
        "from facenet_pytorch import MTCNN\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing my Images"
      ],
      "metadata": {
        "id": "jfS4Q-iI5hGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defining file paths for each set used in data\n",
        "pos_training = \"/content/drive/MyDrive/Project2_Data/positives/training\"\n",
        "pos_test = \"/content/drive/MyDrive/Project2_Data/positives/test\"\n",
        "pos_validation = \"/content/drive/MyDrive/Project2_Data/positives/validation\"\n",
        "\n",
        "neg_training = \"/content/drive/MyDrive/Project2_Data/negatives/training\"\n",
        "neg_test = \"/content/drive/MyDrive/Project2_Data/negatives/test\"\n",
        "neg_validation = \"/content/drive/MyDrive/Project2_Data/negatives/validation\"\n",
        "\n",
        "folders = {\n",
        "    \"positives_training\": pos_training,\n",
        "    \"positives_test\": pos_test,\n",
        "    \"positives_validation\": pos_validation,\n",
        "    \"negatives_training\": neg_training,\n",
        "    \"negatives_test\": neg_test,\n",
        "    \"negatives_validation\": neg_validation,\n",
        "}\n"
      ],
      "metadata": {
        "id": "kGYcP-ROkkE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#output folder to store results\n",
        "base_output = \"/content/drive/MyDrive/Project2_Data/aligned\"\n",
        "os.makedirs(base_output, exist_ok=True)\n",
        "for name in folders.keys():\n",
        "    os.makedirs(os.path.join(base_output, name), exist_ok=True)\n",
        "\n",
        "#defining MTCNN\n",
        "mtcnn = MTCNN(image_size=160, margin=10, keep_all=False, device='cpu')\n",
        "\n",
        "def align_faces(input_folder, output_folder):\n",
        "    print(f\"\\nProcessing: {input_folder}\")\n",
        "    for filename in tqdm(os.listdir(input_folder)):\n",
        "        if filename.startswith('.'):\n",
        "            continue  # skip hidden files\n",
        "        filepath = os.path.join(input_folder, filename)\n",
        "        if not os.path.isfile(filepath):\n",
        "            continue\n",
        "        try:\n",
        "            img = Image.open(filepath).convert(\"RGB\")\n",
        "            face = mtcnn(img)\n",
        "            if face is not None:\n",
        "                # Convert back to image\n",
        "                aligned_img = Image.fromarray((face.permute(1, 2, 0).numpy() * 255).astype('uint8'))\n",
        "                save_path = os.path.join(output_folder, filename)\n",
        "                aligned_img.save(save_path)\n",
        "            else:\n",
        "                print(f\"No face detected in {filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "#performs function on each folder\n",
        "for name, path in folders.items():\n",
        "    out_path = os.path.join(base_output, name)\n",
        "    align_faces(path, out_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkDRO8oI5u-4",
        "outputId": "2c6b34b2-d499-4d94-b5da-fc663f0c64e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing: /content/drive/MyDrive/Project2_Data/positives/training\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [06:21<00:00, 11.91s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing: /content/drive/MyDrive/Project2_Data/positives/test\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 13/13 [02:27<00:00, 11.37s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing: /content/drive/MyDrive/Project2_Data/positives/validation\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [02:14<00:00, 11.23s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing: /content/drive/MyDrive/Project2_Data/negatives/training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [03:16<00:00,  7.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing: /content/drive/MyDrive/Project2_Data/negatives/test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [01:32<00:00,  7.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing: /content/drive/MyDrive/Project2_Data/negatives/validation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [01:15<00:00,  6.84s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Models"
      ],
      "metadata": {
        "id": "RB5DYNo7RKOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings + Threshold"
      ],
      "metadata": {
        "id": "WqdfJHQ4rjYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from facenet_pytorch import InceptionResnetV1\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "# Load pretrained FaceNet model\n",
        "resnet = InceptionResnetV1(pretrained='vggface2').eval()\n",
        "\n",
        "# Transform for model input\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "#function to create enbedding for each image, or numerical representation of each image\n",
        "def get_embedding(image_path):\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img_tensor = transform(img).unsqueeze(0)  # shape (1,3,160,160)\n",
        "    with torch.no_grad():\n",
        "        embedding = resnet(img_tensor)\n",
        "    return embedding.squeeze().numpy()  # shape (512,)\n"
      ],
      "metadata": {
        "id": "v0uJWr4LRNQl"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#new aligned file paths\n",
        "p_aligned_training = \"/content/drive/MyDrive/Project2_Data/aligned/positives_training\"\n",
        "p_aligned_test = \"/content/drive/MyDrive/Project2_Data/aligned/positives_test\"\n",
        "p_aligned_validation = \"/content/drive/MyDrive/Project2_Data/aligned/positives_validation\"\n",
        "\n",
        "n_aligned_training = \"/content/drive/MyDrive/Project2_Data/aligned/negatives_training\"\n",
        "n_aligned_test = \"/content/drive/MyDrive/Project2_Data/aligned/negatives_test\"\n",
        "n_aligned_validation = \"/content/drive/MyDrive/Project2_Data/aligned/negatives_validation\""
      ],
      "metadata": {
        "id": "ugba2iCMkznK"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper: extract embeddings from all images in a folder\n",
        "def get_embeddings_from_folder(folder_path, label):\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "\n",
        "    for filename in tqdm(os.listdir(folder_path), desc=f\"Processing {os.path.basename(folder_path)}\"):\n",
        "        filepath = os.path.join(folder_path, filename)\n",
        "        if os.path.isfile(filepath) and not filename.startswith('.'):\n",
        "            try:\n",
        "                emb = get_embedding(filepath)\n",
        "                embeddings.append(emb)\n",
        "                labels.append(label)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filepath}: {e}\")\n",
        "    return embeddings, labels\n",
        "\n",
        "# Process all splits\n",
        "X_train_p, y_train_p = get_embeddings_from_folder(p_aligned_training, 1)\n",
        "X_train_n, y_train_n = get_embeddings_from_folder(n_aligned_training, 0)\n",
        "\n",
        "X_val_p, y_val_p = get_embeddings_from_folder(p_aligned_validation, 1)\n",
        "X_val_n, y_val_n = get_embeddings_from_folder(n_aligned_validation, 0)\n",
        "\n",
        "# Combine each split\n",
        "X_train = np.array(X_train_p + X_train_n)\n",
        "y_train = np.array(y_train_p + y_train_n)\n",
        "\n",
        "X_val = np.array(X_val_p + X_val_n)\n",
        "y_val = np.array(y_val_p + y_val_n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38yo9JI_l9tX",
        "outputId": "c97bac1c-1490-4827-ca4e-55965ef87e3e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing positives_training: 100%|██████████| 31/31 [00:03<00:00,  9.10it/s]\n",
            "Processing negatives_training: 100%|██████████| 24/24 [00:03<00:00,  6.12it/s]\n",
            "Processing positives_validation: 100%|██████████| 11/11 [00:01<00:00,  8.36it/s]\n",
            "Processing negatives_validation: 100%|██████████| 10/10 [00:01<00:00,  9.36it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape, y_train.shape)\n",
        "print(X_val.shape, y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5h5WqgJnmOMf",
        "outputId": "6d56a6df-41ab-4e2d-9fbc-da4d7f9d9a6d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(55, 512) (55,)\n",
            "(21, 512) (21,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import classification_report, roc_curve\n",
        "\n",
        "#this reference embedding will be used to give new images reference point to average embedding of positive images\n",
        "anchor_embedding = np.mean(X_train[y_train == 1], axis=0)\n",
        "\n",
        "#calculating the cosine similarities of each image in the validation set\n",
        "similarities = cosine_similarity(X_val, anchor_embedding.reshape(1, -1)).flatten()\n",
        "\n",
        "# Validation similarities\n",
        "val_sim = cosine_similarity(X_val, anchor_embedding.reshape(1, -1)).flatten()\n",
        "\n",
        "# Compute ROC to find best threshold\n",
        "fpr, tpr, thresholds = roc_curve(y_val, val_sim)\n",
        "best_idx = np.argmax(tpr - fpr)\n",
        "best_threshold = thresholds[best_idx]\n",
        "\n",
        "print(f\"Best threshold = {best_threshold:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrkkSQINovzS",
        "outputId": "c2d1c82e-bb1d-427c-a92d-a56a3c4dfbe3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best threshold = 0.768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = (similarities >= best_threshold).astype(int)\n",
        "\n",
        "print(classification_report(y_val, y_pred, digits=3, target_names=['Negative', 'Positive']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBTFGOC_pLDq",
        "outputId": "b32052b4-a123-47d9-eb94-eded8dbba7b2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative      0.636     0.700     0.667        10\n",
            "    Positive      0.700     0.636     0.667        11\n",
            "\n",
            "    accuracy                          0.667        21\n",
            "   macro avg      0.668     0.668     0.667        21\n",
            "weighted avg      0.670     0.667     0.667        21\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN Classifier"
      ],
      "metadata": {
        "id": "7ZOc-Xg8rqbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "#specifying image size to be used on CNN\n",
        "IMG_SIZE = (160, 160)\n",
        "\n",
        "#function to load each image from the respective files\n",
        "def load_images_from_folder(folder, label):\n",
        "    images, labels = [], []\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            path = os.path.join(folder, filename)\n",
        "            img = load_img(path, target_size=IMG_SIZE)\n",
        "            img = img_to_array(img) / 255.0\n",
        "            images.append(img)\n",
        "            labels.append(label)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "X_pos_train, y_pos_train = load_images_from_folder(p_aligned_training, 1)\n",
        "X_neg_train, y_neg_train = load_images_from_folder(n_aligned_training, 0)\n",
        "\n",
        "# Combine and shuffle\n",
        "X_train = np.concatenate([X_pos_train, X_neg_train])\n",
        "y_train = np.concatenate([y_pos_train, y_neg_train])\n",
        "\n",
        "# Repeat for validation/test\n",
        "X_pos_val, y_pos_val = load_images_from_folder(p_aligned_validation, 1)\n",
        "X_neg_val, y_neg_val = load_images_from_folder(n_aligned_validation, 0)\n",
        "\n",
        "X_val = np.concatenate([X_pos_val, X_neg_val])\n",
        "y_val = np.concatenate([y_pos_val, y_neg_val])\n",
        "\n",
        "X_pos_test, y_pos_test = load_images_from_folder(p_aligned_test, 1)\n",
        "X_neg_test, y_neg_test = load_images_from_folder(n_aligned_test, 0)\n",
        "\n",
        "X_test = np.concatenate([X_pos_test, X_neg_test])\n",
        "y_test = np.concatenate([y_pos_test, y_neg_test])"
      ],
      "metadata": {
        "id": "Nz9Uz3sT92rZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#uses augmentation to increase diversity of training set\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow(X_train, y_train, batch_size=16)\n",
        "val_generator = train_datagen.flow(X_val, y_val, batch_size=16)\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "#creating layers of CNN with convolution, pooling, and dense layers\n",
        "inputs = keras.Input(shape=(160, 160, 3))\n",
        "x = layers.Conv2D(32, (3,3), activation='relu')(inputs)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "x = layers.Conv2D(64, (3,3), activation='relu')(x)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "x = layers.Conv2D(128, (3,3), activation='relu')(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(128, activation='relu')(x)\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "#defining model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#fitting model\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=20\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGEdkBn5_1mq",
        "outputId": "dd3b5204-75fb-40b0-cdde-667d7259398b",
        "collapsed": true
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.5108 - loss: 0.7140 - val_accuracy: 0.4762 - val_loss: 0.6613\n",
            "Epoch 2/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - accuracy: 0.6867 - loss: 0.6780 - val_accuracy: 0.4286 - val_loss: 0.6941\n",
            "Epoch 3/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.6472 - loss: 0.6784 - val_accuracy: 0.5238 - val_loss: 0.6706\n",
            "Epoch 4/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.5855 - loss: 0.6755 - val_accuracy: 0.5238 - val_loss: 0.6846\n",
            "Epoch 5/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - accuracy: 0.5747 - loss: 0.6634 - val_accuracy: 0.5238 - val_loss: 0.7095\n",
            "Epoch 6/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.5609 - loss: 0.6785 - val_accuracy: 0.6190 - val_loss: 0.6032\n",
            "Epoch 7/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.6617 - loss: 0.6628 - val_accuracy: 0.4762 - val_loss: 0.6973\n",
            "Epoch 8/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.6422 - loss: 0.6557 - val_accuracy: 0.5238 - val_loss: 0.6751\n",
            "Epoch 9/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - accuracy: 0.6308 - loss: 0.6458 - val_accuracy: 0.5714 - val_loss: 0.6255\n",
            "Epoch 10/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.5775 - loss: 0.6667 - val_accuracy: 0.5238 - val_loss: 0.6012\n",
            "Epoch 11/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.7034 - loss: 0.5939 - val_accuracy: 0.5714 - val_loss: 0.5692\n",
            "Epoch 12/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - accuracy: 0.5808 - loss: 0.6364 - val_accuracy: 0.4762 - val_loss: 0.8434\n",
            "Epoch 13/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.5777 - loss: 0.6826 - val_accuracy: 0.5714 - val_loss: 0.6412\n",
            "Epoch 14/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2s/step - accuracy: 0.6056 - loss: 0.6771 - val_accuracy: 0.5714 - val_loss: 0.8778\n",
            "Epoch 15/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.6498 - loss: 0.5644 - val_accuracy: 0.6190 - val_loss: 0.6046\n",
            "Epoch 16/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.5960 - loss: 0.6099 - val_accuracy: 0.6667 - val_loss: 0.6169\n",
            "Epoch 17/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - accuracy: 0.6535 - loss: 0.6326 - val_accuracy: 0.7143 - val_loss: 0.5905\n",
            "Epoch 18/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.5959 - loss: 0.6501 - val_accuracy: 0.6190 - val_loss: 0.7380\n",
            "Epoch 19/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.6090 - loss: 0.6362 - val_accuracy: 0.5238 - val_loss: 0.6701\n",
            "Epoch 20/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.5354 - loss: 0.6828 - val_accuracy: 0.6190 - val_loss: 0.6525\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e4858b6b260>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_proba = model.predict(val_generator)\n",
        "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
        "\n",
        "print(classification_report(y_val, y_pred, target_names=['Negative', 'Positive']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JV7spHC28LBE",
        "outputId": "6bcc7f8f-fb7e-49af-8894-04a6954d21a0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7e4858912f20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.83      0.50      0.62        10\n",
            "    Positive       0.67      0.91      0.77        11\n",
            "\n",
            "    accuracy                           0.71        21\n",
            "   macro avg       0.75      0.70      0.70        21\n",
            "weighted avg       0.75      0.71      0.70        21\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A key consideration in model selection for my selected use case was the recall performance and false positive rate, as I would prefer my model be more secure in detecting negatives rather than select positive correctly everytime. Given this, I will be selecting the Pretrained Embeddings + Baseline model. Please refer to report for further discussion of this decision."
      ],
      "metadata": {
        "id": "M0BfWD9VVWpY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Pretrained Embeddings + Baseline on Test Set"
      ],
      "metadata": {
        "id": "B7xp-TXCErBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#need to reprocess splits\n",
        "X_train_p, y_train_p = get_embeddings_from_folder(p_aligned_training, 1)\n",
        "X_train_n, y_train_n = get_embeddings_from_folder(n_aligned_training, 0)\n",
        "\n",
        "X_test_p, y_test_p = get_embeddings_from_folder(p_aligned_test, 1)\n",
        "X_test_n, y_test_n = get_embeddings_from_folder(n_aligned_test, 0)\n",
        "\n",
        "#recombining splits\n",
        "X_train = np.array(X_train_p + X_train_n)\n",
        "y_train = np.array(y_train_p + y_train_n)\n",
        "\n",
        "X_test = np.array(X_test_p + X_test_n)\n",
        "y_test = np.array(y_test_p + y_test_n)\n",
        "\n",
        "#training on same data that was used on validation set\n",
        "anchor_embedding = np.mean(X_train[y_train == 1], axis=0)\n",
        "\n",
        "#calculating similarities on test set\n",
        "similarities = cosine_similarity(X_test, anchor_embedding.reshape(1, -1)).flatten()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DkcM7A7FnTh",
        "outputId": "1fd14f63-3ccb-4d20-fecf-0f10b8715d43"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing positives_training: 100%|██████████| 31/31 [00:04<00:00,  6.44it/s]\n",
            "Processing negatives_training: 100%|██████████| 24/24 [00:02<00:00,  9.36it/s]\n",
            "Processing positives_test: 100%|██████████| 12/12 [00:01<00:00,  9.29it/s]\n",
            "Processing negatives_test: 100%|██████████| 11/11 [00:01<00:00,  9.29it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = (similarities >= best_threshold).astype(int)\n",
        "\n",
        "print(classification_report(y_test, y_pred, digits=3, target_names=['Negative', 'Positive']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8K34UyfeGfr2",
        "outputId": "f4402a82-7c26-47cd-8b91-584097933e6f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative      0.600     0.818     0.692        11\n",
            "    Positive      0.750     0.500     0.600        12\n",
            "\n",
            "    accuracy                          0.652        23\n",
            "   macro avg      0.675     0.659     0.646        23\n",
            "weighted avg      0.678     0.652     0.644        23\n",
            "\n"
          ]
        }
      ]
    }
  ]
}